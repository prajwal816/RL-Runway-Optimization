{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# graph_from_model.py\n\"\"\"\nEvaluate trained DQN model ONLY and generate success probability graphs.\nThis file is SELF-CONTAINED (includes Env + QNetwork).\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n# =====================================================\n# 1) ENVIRONMENT (copied exactly from your training code)\n# =====================================================\nclass FlightEnv(gym.Env):\n    def __init__(self, start_alt=400.0, start_dist=800.0):\n        super().__init__()\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0, -30, 0], dtype=np.float32),\n            high=np.array([5000, 300, 10000, 30, 1], dtype=np.float32),\n            dtype=np.float32\n        )\n        self.action_space = spaces.Discrete(5)\n        self.start_alt = start_alt\n        self.start_dist = start_dist\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.altitude = float(self.start_alt)\n        self.speed = float(160 + np.random.uniform(-10, 10))\n        self.distance = float(self.start_dist)\n        self.prev_distance = self.distance\n        self.angle = float(np.random.uniform(-2, 2))\n        self.runway_condition = float(np.random.choice([0.0, 0.5, 1.0]))\n        self.steps = 0\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.steps += 1\n\n        if action == 0: self.speed += 6\n        elif action == 1: self.speed -= 6\n        elif action == 2: self.altitude += 35; self.angle += 1.5\n        elif action == 3: self.altitude -= 35; self.angle -= 1.5\n\n        self.distance -= max(self.speed * 0.3, 1)\n        self.altitude -= 8\n\n        drag = {0.0:0.6, 0.5:0.4, 1.0:0.25}[self.runway_condition]\n        self.speed -= drag\n\n        self.angle = np.clip(self.angle, -30, 30)\n        self.altitude = max(self.altitude, 0)\n        self.speed = np.clip(self.speed, 0, 300)\n\n        reward = 0\n        reward += (self.prev_distance - self.distance) * 0.02\n        self.prev_distance = self.distance\n        reward -= 0.03\n        reward -= 0.005 * abs(self.altitude - 100)\n        reward -= 0.005 * abs(self.speed - 150)\n        reward -= 0.01 * abs(self.angle)\n        if self.distance < 400: reward += 0.8\n        if 0 < self.altitude < 100 and 100 < self.speed < 200: reward += 1.5\n        reward += (self.start_dist - self.distance) / self.start_dist\n\n        done, success = False, False\n        outcome = \"in-flight\"\n\n        if self.distance <= 0:\n            if 0 <= self.altitude <= 50 and 100 <= self.speed <= 200 and abs(self.angle) < 10:\n                reward += 200; success = True; outcome = \"successful landing\"\n            else:\n                reward -= 40; outcome = \"failed landing\"\n            done = True\n\n        elif self.altitude <= 0:\n            reward -= 40; done = True; outcome = \"crash before runway\"\n\n        elif self.speed <= 20 and self.altitude > 100:\n            reward -= 40; done = True; outcome = \"stall midair\"\n\n        elif self.steps >= 600:\n            done = True; outcome = \"timeout\"\n\n        return self._get_obs(), reward, done, False, {\n            \"success\": success, \"outcome\": outcome, \"runway\": self.runway_condition\n        }\n\n    def _get_obs(self):\n        return np.array([\n            self.altitude / 5000,\n            self.speed / 300,\n            self.distance / 10000,\n            (self.angle + 30) / 60,\n            self.runway_condition\n        ], dtype=np.float32)\n\n# =====================================================\n# 2) Q-NETWORK (copied exactly from training code)\n# =====================================================\nclass QNetwork(nn.Module):\n    def __init__(self, obs_dim, n_actions):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(obs_dim, 256), nn.ReLU(),\n            nn.Linear(256, 256), nn.ReLU(),\n            nn.Linear(256, n_actions)\n        )\n    def forward(self, x): return self.model(x)\n\n# =====================================================\n# 3) LOAD MODEL\n# =====================================================\nMODEL_PATH = \"/kaggle/input/dqn-flight/pytorch/default/1/best_model (1).pt\"\nEVAL_EPISODES = 20000\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nenv = FlightEnv()\nobs_dim = env.observation_space.shape[0]\nn_actions = env.action_space.n\n\nmodel = QNetwork(obs_dim, n_actions).to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\n\nprint(\"Loaded model from:\", MODEL_PATH)\n\n# =====================================================\n# 4) EVALUATION LOOP\n# =====================================================\nrunway_counts = {0.0:0, 0.5:0, 1.0:0}\nrunway_success = {0.0:0, 0.5:0, 1.0:0}\n\nsuccess_curve = {0.0:[], 0.5:[], 1.0:[]}\nmean_curve = []\n\ndef get_action(obs):\n    with torch.no_grad():\n        t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n        return int(model(t).argmax().item())\n\nfor ep in range(1, EVAL_EPISODES + 1):\n    obs, _ = env.reset()\n    done = False\n    rwy = env.runway_condition\n\n    while not done:\n        action = get_action(obs)\n        obs, _, done, _, info = env.step(action)\n\n    runway_counts[rwy] += 1\n    if info[\"success\"]:\n        runway_success[rwy] += 1\n\n    # Append curves\n    for r in [0.0, 0.5, 1.0]:\n        if runway_counts[r] > 0:\n            success_curve[r].append(runway_success[r] / runway_counts[r])\n        else:\n            success_curve[r].append(0.0)\n\n    mean_curve.append(np.mean([\n        success_curve[0.0][-1],\n        success_curve[0.5][-1],\n        success_curve[1.0][-1]\n    ]))\n\n    if ep % 5000 == 0:\n        print(f\"Progress: {ep}/{EVAL_EPISODES}\")\n\n# =====================================================\n# 5) PLOT RESULTS\n# =====================================================\nepisodes = np.arange(1, EVAL_EPISODES + 1)\n\nplt.figure(figsize=(12,6))\nplt.plot(episodes, success_curve[0.0], label=\"Dry (0.0)\")\nplt.plot(episodes, success_curve[0.5], label=\"Wet (0.5)\")\nplt.plot(episodes, success_curve[1.0], label=\"Icy (1.0)\")\nplt.xlabel(\"Evaluation Episodes\")\nplt.ylabel(\"Success Probability\")\nplt.title(\"Landing Success Probability vs Evaluation Episodes\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(12,6))\nplt.plot(episodes, mean_curve, color=\"black\")\nplt.xlabel(\"Evaluation Episodes\")\nplt.ylabel(\"Mean Success Probability\")\nplt.title(\"Overall Mean Landing Success Across Runways\")\nplt.grid(True)\nplt.show()\n\nprint(\"\\nFinal success probabilities:\")\nprint({k: runway_success[k]/runway_counts[k] for k in runway_counts})\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}