{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train_sac_discrete_full.py\n\"\"\"\nSoft Actor–Critic (Discrete) training script for Flight Landing.\n- SAME structure as original DQN version\n- Per-episode runways, success probability\n- Summaries every 50 episodes\n- SAC: categorical actor, twin critics, soft target updates, auto-entropy tuning\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n# =====================================================\n# 1️⃣ ENVIRONMENT (unchanged)\n# =====================================================\nclass FlightEnv(gym.Env):\n    \"\"\"Simplified Flight Landing Environment with reachable start & runway conditions.\"\"\"\n    def __init__(self, start_alt=400.0, start_dist=800.0):\n        super().__init__()\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0, -30, 0], dtype=np.float32),\n            high=np.array([5000, 300, 10000, 30, 1], dtype=np.float32),\n            dtype=np.float32\n        )\n        self.action_space = spaces.Discrete(5)\n        self.start_alt = start_alt\n        self.start_dist = start_dist\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.altitude = float(self.start_alt)\n        self.speed = float(160.0 + np.random.uniform(-10, 10))\n        self.distance = float(self.start_dist)\n        self.prev_distance = self.distance\n        self.angle = float(np.random.uniform(-2, 2))\n        self.runway_condition = float(np.random.choice([0.0, 0.5, 1.0]))\n        self.steps = 0\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.steps += 1\n\n        # --- Action effects ---\n        if action == 0:  # throttle up\n            self.speed += 6.0\n        elif action == 1:  # throttle down\n            self.speed -= 6.0\n        elif action == 2:  # pitch up\n            self.altitude += 35.0\n            self.angle += 1.5\n        elif action == 3:  # pitch down\n            self.altitude -= 35.0\n            self.angle -= 1.5\n\n        # --- Dynamics ---\n        self.distance -= max(self.speed * 0.3, 1.0)\n        self.altitude -= 8.0\n\n        # --- Runway condition physics ---\n        if self.runway_condition == 0.0:\n            drag_factor = 0.6\n        elif self.runway_condition == 0.5:\n            drag_factor = 0.4\n        else:\n            drag_factor = 0.25\n\n        self.speed -= drag_factor\n        self.angle = np.clip(self.angle, -30, 30)\n        self.altitude = max(self.altitude, 0.0)\n        self.speed = np.clip(self.speed, 0.0, 300.0)\n\n        # --- Reward shaping ---\n        reward = 0.0\n        reward += (self.prev_distance - self.distance) * 0.02\n        self.prev_distance = self.distance\n\n        reward -= 0.03\n        reward -= 0.005 * abs(self.altitude - 100)\n        reward -= 0.005 * abs(self.speed - 150)\n        reward -= 0.01 * abs(self.angle)\n\n        if self.distance < 400:\n            reward += 0.8\n        if 0 < self.altitude < 100 and 100 < self.speed < 200:\n            reward += 1.5\n\n        reward += (self.start_dist - self.distance) / max(1.0, self.start_dist)\n\n        done = False\n        success = False\n        outcome = \"in-flight\"\n\n        # Landing logic\n        if self.distance <= 0:\n            if 0 <= self.altitude <= 50 and 100 <= self.speed <= 200 and abs(self.angle) < 10:\n                reward += 200.0\n                success = True\n                outcome = \"successful landing\"\n            else:\n                reward -= 40.0\n                outcome = \"failed landing\"\n            done = True\n\n        # Crash / stall / timeout\n        if self.altitude <= 0 and self.distance > 0:\n            reward -= 40.0\n            done = True\n            outcome = \"crash before runway\"\n        if self.speed <= 20 and self.altitude > 100:\n            reward -= 40.0\n            done = True\n            outcome = \"stall midair\"\n        if self.steps >= 600:\n            done = True\n            outcome = \"timeout\"\n\n        info = {\n            \"success\": success,\n            \"outcome\": outcome,\n            \"runway_condition\": self.runway_condition\n        }\n        return self._get_obs(), float(reward), bool(done), False, info\n\n    def _get_obs(self):\n        return np.array([\n            self.altitude / 5000.0,\n            self.speed / 300.0,\n            self.distance / 10000.0,\n            (self.angle + 30.0) / 60.0,\n            self.runway_condition\n        ], dtype=np.float32)\n\n\n# =====================================================\n# 2️⃣ REPLAY BUFFER (unchanged)\n# =====================================================\nclass ReplayBuffer:\n    def __init__(self, capacity: int, obs_shape):\n        self.capacity = int(capacity)\n        self.obs_shape = tuple(obs_shape)\n        self.ptr = 0\n        self.size = 0\n        self.states = np.zeros((self.capacity,) + self.obs_shape, dtype=np.float32)\n        self.next_states = np.zeros((self.capacity,) + self.obs_shape, dtype=np.float32)\n        self.actions = np.zeros((self.capacity,), dtype=np.int64)\n        self.rewards = np.zeros((self.capacity,), dtype=np.float32)\n        self.dones = np.zeros((self.capacity,), dtype=np.float32)\n\n    def push(self, state, action, reward, next_state, done):\n        self.states[self.ptr] = state\n        self.next_states[self.ptr] = next_state\n        self.actions[self.ptr] = int(action)\n        self.rewards[self.ptr] = float(reward)\n        self.dones[self.ptr] = 1.0 if done else 0.0\n        self.ptr = (self.ptr + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n\n    def sample(self, batch_size: int):\n        idxs = np.random.randint(0, self.size, size=batch_size)\n        return dict(\n            states=self.states[idxs],\n            actions=self.actions[idxs],\n            rewards=self.rewards[idxs],\n            next_states=self.next_states[idxs],\n            dones=self.dones[idxs]\n        )\n\n    def __len__(self):\n        return self.size\n\n\n# =====================================================\n# 3️⃣ SAC AGENT (Discrete)\n# =====================================================\nclass MLP(nn.Module):\n    def __init__(self, inp, out, hidden=(256, 256)):\n        super().__init__()\n        layers = []\n        last = inp\n        for h in hidden:\n            layers += [nn.Linear(last, h), nn.ReLU()]\n            last = h\n        layers.append(nn.Linear(last, out))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Actor(nn.Module):\n    \"\"\"Categorical policy for discrete SAC.\"\"\"\n    def __init__(self, obs_dim, n_actions):\n        super().__init__()\n        self.net = MLP(obs_dim, n_actions)\n\n    def forward(self, obs):\n        return self.net(obs)  # logits\n\n\nclass Critic(nn.Module):\n    def __init__(self, obs_dim, n_actions):\n        super().__init__()\n        self.net = MLP(obs_dim, n_actions)\n\n    def forward(self, obs):\n        return self.net(obs)  # Q-values\n\n\nclass SACDiscrete:\n    def __init__(self, obs_dim, n_actions, lr=3e-4, gamma=0.99,\n                 tau=0.005, alpha=0.1, device='cpu'):\n\n        self.device = torch.device(device)\n        self.gamma = gamma\n        self.tau = tau\n        self.n_actions = n_actions\n\n        # Actor\n        self.actor = Actor(obs_dim, n_actions).to(self.device)\n        self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr)\n\n        # Critics\n        self.q1 = Critic(obs_dim, n_actions).to(self.device)\n        self.q2 = Critic(obs_dim, n_actions).to(self.device)\n        self.q1_opt = optim.Adam(self.q1.parameters(), lr=lr)\n        self.q2_opt = optim.Adam(self.q2.parameters(), lr=lr)\n\n        # Target critics\n        self.q1_t = Critic(obs_dim, n_actions).to(self.device)\n        self.q2_t = Critic(obs_dim, n_actions).to(self.device)\n        self.q1_t.load_state_dict(self.q1.state_dict())\n        self.q2_t.load_state_dict(self.q2.state_dict())\n\n        # Entropy temperature\n        self.log_alpha = torch.tensor(np.log(alpha), requires_grad=True, device=self.device)\n        self.alpha_opt = optim.Adam([self.log_alpha], lr=lr)\n        self.target_entropy = -np.log(1.0 / n_actions) * 0.98\n\n        self.mse = nn.MSELoss()\n\n    @property\n    def alpha(self):\n        return self.log_alpha.exp()\n\n    def act(self, obs):\n        t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n        logits = self.actor(t)\n        probs = torch.softmax(logits, dim=-1)\n        dist = torch.distributions.Categorical(probs)\n        return int(dist.sample().item())\n\n    def update(self, batch):\n        s = torch.tensor(batch[\"states\"], device=self.device, dtype=torch.float32)\n        ns = torch.tensor(batch[\"next_states\"], device=self.device, dtype=torch.float32)\n        a = torch.tensor(batch[\"actions\"], device=self.device, dtype=torch.int64)\n        r = torch.tensor(batch[\"rewards\"], device=self.device, dtype=torch.float32)\n        d = torch.tensor(batch[\"dones\"], device=self.device, dtype=torch.float32)\n\n        # ---------- Compute target Q ----------\n        with torch.no_grad():\n            next_logits = self.actor(ns)\n            next_probs = torch.softmax(next_logits, dim=-1)\n            next_logp = torch.log(next_probs + 1e-12)\n\n            q1_t = self.q1_t(ns)\n            q2_t = self.q2_t(ns)\n            q_min = torch.min(q1_t, q2_t)\n\n            v_next = (next_probs * (q_min - self.alpha.detach() * next_logp)).sum(dim=1)\n\n            target_q = r + (1 - d) * self.gamma * v_next\n\n        # ---------- Critic losses ----------\n        q1_vals = self.q1(s).gather(1, a.unsqueeze(1)).squeeze(1)\n        q2_vals = self.q2(s).gather(1, a.unsqueeze(1)).squeeze(1)\n\n        loss_q1 = self.mse(q1_vals, target_q)\n        loss_q2 = self.mse(q2_vals, target_q)\n\n        self.q1_opt.zero_grad()\n        loss_q1.backward()\n        nn.utils.clip_grad_norm_(self.q1.parameters(), 10.0)\n        self.q1_opt.step()\n\n        self.q2_opt.zero_grad()\n        loss_q2.backward()\n        nn.utils.clip_grad_norm_(self.q2.parameters(), 10.0)\n        self.q2_opt.step()\n\n        # ---------- Actor loss ----------\n        logits = self.actor(s)\n        probs = torch.softmax(logits, dim=-1)\n        logp = torch.log(probs + 1e-12)\n        q1_vals = self.q1(s)\n        q2_vals = self.q2(s)\n        q_min = torch.min(q1_vals, q2_vals)\n\n        actor_loss = (probs * (self.alpha.detach() * logp - q_min)).sum(dim=1).mean()\n\n        self.actor_opt.zero_grad()\n        actor_loss.backward()\n        nn.utils.clip_grad_norm_((self.actor.parameters()), 10.0)\n        self.actor_opt.step()\n\n        # ---------- Alpha (entropy temperature) update ----------\n        entropy = -(probs * logp).sum(dim=1).mean()\n        alpha_loss = -(self.log_alpha * (entropy - self.target_entropy).detach())\n\n        self.alpha_opt.zero_grad()\n        alpha_loss.backward()\n        self.alpha_opt.step()\n\n        # ---------- Soft target update ----------\n        with torch.no_grad():\n            for p, tp in zip(self.q1.parameters(), self.q1_t.parameters()):\n                tp.data.mul_(1 - self.tau)\n                tp.data.add_(self.tau * p.data)\n            for p, tp in zip(self.q2.parameters(), self.q2_t.parameters()):\n                tp.data.mul_(1 - self.tau)\n                tp.data.add_(self.tau * p.data)\n\n\n# =====================================================\n# 4️⃣ TRAINING (same structure as DQN version)\n# =====================================================\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument('--episodes', type=int, default=30000)\n    p.add_argument('--lr', type=float, default=3e-4)\n    p.add_argument('--buffer_size', type=int, default=30000)\n    p.add_argument('--batch_size', type=int, default=128)\n    p.add_argument('--save_dir', type=str, default='./checkpoints')\n    p.add_argument('--cpu', action='store_true')\n    p.add_argument('--seed', type=int, default=42)\n    return p.parse_args()\n\n\ndef train(args):\n    device = 'cuda' if torch.cuda.is_available() and not args.cpu else 'cpu'\n    torch.manual_seed(args.seed); np.random.seed(args.seed); random.seed(args.seed)\n\n    env = FlightEnv()\n    obs_dim = env.observation_space.shape[0]\n    n_actions = env.action_space.n\n\n    agent = SACDiscrete(obs_dim, n_actions, lr=args.lr, device=device)\n    buffer = ReplayBuffer(args.buffer_size, (obs_dim,))\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    outcomes = {\"successful landing\": 0, \"failed landing\": 0,\n                \"crash before runway\": 0, \"stall midair\": 0, \"timeout\": 0}\n    runway_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n    runway_success = {0.0: 0, 0.5: 0, 1.0: 0}\n\n    best_return = -1e9\n\n    for ep in range(1, args.episodes + 1):\n        obs, _ = env.reset()\n        ep_return = 0.0\n        done = False\n        outcome = None\n        runway = env.runway_condition\n\n        while not done:\n            action = agent.act(obs)\n            nobs, rew, done, _, info = env.step(action)\n\n            buffer.push(obs, action, rew, nobs, done)\n            obs = nobs\n            ep_return += rew\n            outcome = info[\"outcome\"]\n\n            if len(buffer) > args.batch_size:\n                batch = buffer.sample(args.batch_size)\n                agent.update(batch)\n\n        # Aggregate statistics\n        outcomes[outcome] += 1\n        runway_counts[runway] += 1\n        if info[\"success\"]:\n            runway_success[runway] += 1\n\n        runway_probs = {\n            k: (runway_success[k] / runway_counts[k] if runway_counts[k] > 0 else 0)\n            for k in runway_counts\n        }\n\n        print(f\"Ep {ep}/{args.episodes} | Runway: {runway} | Return: {ep_return:.2f} \"\n              f\"| Outcome: {outcome} | Success Prob: {runway_probs[runway]:.2f}\")\n\n        if ep_return > best_return:\n            best_return = ep_return\n            torch.save(agent.actor.state_dict(), os.path.join(args.save_dir, \"best_actor.pt\"))\n            torch.save(agent.q1.state_dict(), os.path.join(args.save_dir, \"best_q1.pt\"))\n            torch.save(agent.q2.state_dict(), os.path.join(args.save_dir, \"best_q2.pt\"))\n\n        if ep % 5000 == 0:\n            print(\"\\n--- Outcome Summary up to Episode\", ep, \"---\")\n            for k, v in outcomes.items():\n                print(f\"{k:<25}: {v}\")\n            print(\"Runway success probabilities:\", runway_probs)\n            print(\"------------------------------------------\\n\")\n\n    print(\"✅ Training done.\")\n    print(\"Best episode return:\", best_return)\n    print(\"Final outcomes summary:\", outcomes)\n    print(\"Final runway success probabilities:\", runway_probs)\n\n\n# =====================================================\n# 5️⃣ RUN\n# =====================================================\nif __name__ == \"__main__\":\n    import sys\n    sys.argv = [sys.argv[0]]\n    args = parse_args()\n    print(\"Starting SAC-Discrete training with args:\", args)\n    train(args)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}