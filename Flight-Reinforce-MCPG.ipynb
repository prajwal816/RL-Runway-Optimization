{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train_reinforce.py\n\"\"\"\nREINFORCE (Monte-Carlo Policy Gradient) training script for Flight Landing.\n- Self-contained FlightEnv (5-dim observation, 5 discrete actions).\n- Prints per-episode progress and runway type/outcome.\n- Tracks per-runway success probabilities.\n- Saves best policy.\n- Tweaked dynamics & rewards to improve successful landing frequency.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n# -----------------------\n# Environment\n# -----------------------\nclass FlightEnv(gym.Env):\n    def __init__(self, start_alt=300.0, start_dist=600.0):\n        super().__init__()\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0, -30, 0], dtype=np.float32),\n            high=np.array([5000, 300, 10000, 30, 1], dtype=np.float32),\n            dtype=np.float32\n        )\n        self.action_space = spaces.Discrete(5)\n        self.start_alt = start_alt\n        self.start_dist = start_dist\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.altitude = float(self.start_alt)\n        self.speed = float(160.0 + np.random.uniform(-8, 8))\n        self.distance = float(self.start_dist)\n        self.prev_distance = self.distance\n        self.angle = float(np.random.uniform(-2, 2))\n        self.runway_condition = float(np.random.choice([0.0, 0.5, 1.0]))\n        self.steps = 0\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.steps += 1\n        # Actions: 0 throttle+, 1 throttle-, 2 pitch+, 3 pitch-, 4 noop\n        if action == 0:\n            self.speed += 4.0   # gentler throttle increments\n        elif action == 1:\n            self.speed -= 4.0\n        elif action == 2:\n            self.altitude += 25.0\n            self.angle += 1.2\n        elif action == 3:\n            self.altitude -= 25.0\n            self.angle -= 1.2\n        # else 4: maintain\n\n        # --- Dynamics (gentler descent & drag) ---\n        self.distance -= max(self.speed * 0.22, 1.0)  # slower approach per step\n        self.altitude -= 5.0                          # gentler gravity\n        self.speed -= 0.3                             # lighter drag\n        self.angle = np.clip(self.angle, -30, 30)\n        self.altitude = max(self.altitude, 0.0)\n        self.speed = np.clip(self.speed, 0.0, 300.0)\n\n        # --- Reward shaping (stronger, denser feedback) ---\n        reward = 0.0\n        # progress-based reward (larger)\n        reward += (self.prev_distance - self.distance) * 0.05\n        self.prev_distance = self.distance\n\n        # small step penalty (keeps episodes efficient)\n        reward -= 0.02\n\n        # altitude and speed shaping (softer penalties)\n        reward -= 0.003 * abs(self.altitude - 100)\n        reward -= 0.003 * abs(self.speed - 150)\n\n        # pitch penalty\n        reward -= 0.008 * abs(self.angle)\n\n        # near runway bonus (earlier)\n        if self.distance < 500:\n            reward += 1.0\n\n        # bonus for ideal landing profile\n        if 0 < self.altitude < 100 and 100 < self.speed < 200:\n            reward += 2.0\n\n        # proportional progress reward\n        reward += 1.0 * ((self.start_dist - self.distance) / max(1.0, self.start_dist))\n\n        done = False\n        success = False\n        outcome = \"in-flight\"\n\n        # --- Landing event (much stronger landing reward) ---\n        if self.distance <= 0:\n            # success window widened a touch\n            if 0 <= self.altitude <= 60 and 95 <= self.speed <= 210 and abs(self.angle) < 12:\n                reward += 400.0  # boosted landing bonus to strongly reinforce correct touchdown\n                success = True\n                outcome = \"successful landing\"\n            else:\n                reward -= 40.0\n                outcome = \"failed landing\"\n            done = True\n\n        # --- Crash / stall conditions ---\n        if self.altitude <= 0 and self.distance > 0:\n            reward -= 60.0\n            done = True\n            outcome = \"crash before runway\"\n\n        if self.speed <= 20 and self.altitude > 100:\n            reward -= 60.0\n            done = True\n            outcome = \"stall midair\"\n\n        # Allow longer episodes to give the agent more time to correct\n        if self.steps >= 800:\n            done = True\n            outcome = \"timeout\"\n\n        return self._get_obs(), float(reward), bool(done), False, {\"success\": success, \"outcome\": outcome,\n                                                                    \"runway\": self.runway_condition}\n\n    def _get_obs(self):\n        return np.array([\n            self.altitude / 5000.0,\n            self.speed / 300.0,\n            self.distance / 10000.0,\n            (self.angle + 30.0) / 60.0,\n            self.runway_condition\n        ], dtype=np.float32)\n\n\n# -----------------------\n# Policy network\n# -----------------------\nclass PolicyNet(nn.Module):\n    def __init__(self, obs_dim, n_actions, hidden=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, n_actions)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass ValueNet(nn.Module):\n    def __init__(self, obs_dim, hidden=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1)\n\n\n# -----------------------\n# REINFORCE Training\n# -----------------------\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument('--episodes', type=int, default=50000)\n    p.add_argument('--lr', type=float, default=3e-4)\n    p.add_argument('--gamma', type=float, default=0.99)\n    p.add_argument('--hidden', type=int, default=256)\n    p.add_argument('--use_baseline', action='store_true', help='Use value baseline to reduce variance')\n    p.add_argument('--entropy_coef', type=float, default=0.0015)  # slightly larger to encourage exploration early\n    p.add_argument('--save_dir', type=str, default='./checkpoints_reinforce')\n    p.add_argument('--report_every', type=int, default=10)\n    p.add_argument('--seed', type=int, default=42)\n    p.add_argument('--cpu', action='store_true')\n    return p.parse_args()\n\n\n\ndef train(args):\n    device = 'cuda' if torch.cuda.is_available() and not args.cpu else 'cpu'\n    torch.manual_seed(args.seed); np.random.seed(args.seed); random.seed(args.seed)\n\n    env = FlightEnv()\n    obs_dim = env.observation_space.shape[0]\n    n_actions = env.action_space.n\n\n    policy = PolicyNet(obs_dim, n_actions, hidden=args.hidden).to(device)\n    policy_opt = optim.Adam(policy.parameters(), lr=args.lr)\n\n    value, value_opt = None, None\n    if args.use_baseline:\n        value = ValueNet(obs_dim).to(device)\n        value_opt = optim.Adam(value.parameters(), lr=args.lr)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    success_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n    total_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n    outcomes = defaultdict(int)\n\n    best_return = -1e9\n\n    def finish_episode(episode_states, episode_actions, episode_rewards):\n        R = 0.0\n        returns = []\n        for r in reversed(episode_rewards):\n            R = r + args.gamma * R\n            returns.insert(0, R)\n        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n        # normalize returns for stability\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n        states_t = torch.tensor(np.array(episode_states), dtype=torch.float32, device=device)\n        actions_t = torch.tensor(episode_actions, dtype=torch.int64, device=device)\n\n        logits = policy(states_t)\n        logp = torch.log_softmax(logits, dim=1)\n        selected_logp = logp.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n\n        if args.use_baseline:\n            values = value(states_t).detach()\n            adv = returns - (values - values.mean())\n        else:\n            adv = returns\n\n        policy_loss = - (selected_logp * adv).mean()\n        probs = torch.softmax(logits, dim=1)\n        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1).mean()\n        loss = policy_loss - args.entropy_coef * entropy\n\n        policy_opt.zero_grad()\n        loss.backward()\n        policy_opt.step()\n\n        if args.use_baseline:\n            value_opt.zero_grad()\n            v_loss = nn.MSELoss()(value(states_t), returns)\n            v_loss.backward()\n            value_opt.step()\n            return float(loss.item()), float(v_loss.item())\n        return float(loss.item()), None\n\n    # -------------------------------------------------------\n    # Training Loop\n    # -------------------------------------------------------\n    for ep in range(1, args.episodes + 1):\n        obs, _ = env.reset()\n        ep_states, ep_actions, ep_rewards = [], [], []\n        ep_return = 0.0\n        done = False\n        runway = env.runway_condition\n        outcome = \"in-flight\"\n\n        print(f\"Ep {ep}/{args.episodes} ...\", end=\"\", flush=True)\n\n        while not done:\n            state = obs.astype(np.float32)\n            st_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n            with torch.no_grad():\n                logits = policy(st_t)\n                probs = torch.softmax(logits, dim=1).cpu().numpy().squeeze(0)\n            # sample action using the policy distribution\n            action = np.random.choice(n_actions, p=probs)\n\n            next_obs, reward, terminated, truncated, info = env.step(action)\n            done = bool(terminated or truncated)\n\n            ep_states.append(state)\n            ep_actions.append(action)\n            ep_rewards.append(reward)\n\n            obs = next_obs\n            ep_return += reward\n            outcome = info.get(\"outcome\", outcome)\n\n        pl_loss, v_loss = finish_episode(ep_states, ep_actions, ep_rewards)\n\n        total_counts[runway] += 1\n        if info.get(\"success\", False):\n            success_counts[runway] += 1\n            outcomes[\"successful landing\"] += 1\n        else:\n            outcomes[outcome] += 1\n\n        loss_str = f\" | PLoss: {pl_loss:.3f}\"\n        if v_loss is not None:\n            loss_str += f\" | VLoss: {v_loss:.3f}\"\n\n        print(f\" Return: {ep_return:.2f} | Runway: {runway} | Outcome: {outcome}{loss_str}\")\n\n        if ep_return > best_return:\n            best_return = ep_return\n            torch.save(policy.state_dict(), os.path.join(args.save_dir, \"reinforce_best_policy.pt\"))\n            if args.use_baseline:\n                torch.save(value.state_dict(), os.path.join(args.save_dir, \"reinforce_best_value.pt\"))\n\n        # -------------------------------------------------------\n        # Outcome Summary Every 2000 Episodes\n        # -------------------------------------------------------\n        if ep % 2000 == 0:\n            print(f\"\\n--- Outcome Summary up to Episode {ep} ---\")\n            print(f\"successful landing       : {outcomes['successful landing']}\")\n            print(f\"failed landing           : {outcomes['failed landing']}\")\n            print(f\"crash before runway      : {outcomes['crash before runway']}\")\n            print(f\"stall midair             : {outcomes['stall midair']}\")\n            print(f\"timeout                  : {outcomes['timeout']}\")\n\n            succ_rates = {\n                k: (success_counts[k] / total_counts[k] if total_counts[k] > 0 else 0.0)\n                for k in success_counts\n            }\n            print(f\"Runway success probabilities: {succ_rates}\")\n            print(\"------------------------------------------\\n\")\n\n        # Existing periodic report (kept)\n        if ep % args.report_every == 0:\n            succ_rates = {k: (success_counts[k] / total_counts[k] if total_counts[k] > 0 else 0.0) for k in success_counts}\n            print(f\"  --- Report @ Ep {ep}: BestReturn {best_return:.2f} | SuccessRates: {succ_rates}\")\n            print(\"  Outcomes so far:\", dict(outcomes))\n\n\n    # Final save\n    torch.save(policy.state_dict(), os.path.join(args.save_dir, \"reinforce_final_policy.pt\"))\n    if args.use_baseline:\n        torch.save(value.state_dict(), os.path.join(args.save_dir, \"reinforce_final_value.pt\"))\n\n    final_rates = {k: (success_counts[k] / total_counts[k] if total_counts[k] > 0 else 0.0) for k in success_counts}\n    print(\"Training finished. Final success rates by runway:\", final_rates)\n    print(\"Best episode return:\", best_return)\n\n\n\nif __name__ == \"__main__\":\n    import sys\n    sys.argv = [sys.argv[0]]\n    args = parse_args()\n    print(\"Starting REINFORCE training with args:\", args)\n    train(args)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:37:57.529471Z","iopub.execute_input":"2025-11-14T09:37:57.529983Z","execution_failed":"2025-11-14T09:39:19.289Z"}},"outputs":[],"execution_count":null}]}