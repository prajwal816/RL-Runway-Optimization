{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train_ppo_full_fixed.py\n\"\"\"\nPPO Training Script for Flight Landing Environment (fixed bookkeeping).\n- Handles unknown 'outcome' values like \"in-flight\" without KeyError.\n- Ensures runway_condition keys are normalized to Python floats when used as dict keys.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium import spaces\n\nclass FlightEnv(gym.Env):\n    \"\"\"Same FlightEnv as before\"\"\"\n    def __init__(self, start_alt=400.0, start_dist=800.0):\n        super().__init__()\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0, -30, 0], dtype=np.float32),\n            high=np.array([5000, 300, 10000, 30, 1], dtype=np.float32),\n            dtype=np.float32\n        )\n        self.action_space = spaces.Discrete(5)\n        self.start_alt = start_alt\n        self.start_dist = start_dist\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.altitude = float(self.start_alt)\n        self.speed = float(160.0 + np.random.uniform(-10, 10))\n        self.distance = float(self.start_dist)\n        self.prev_distance = self.distance\n        self.angle = float(np.random.uniform(-2, 2))\n        self.runway_condition = float(np.random.choice([0.0, 0.5, 1.0]))\n        self.steps = 0\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.steps += 1\n        if action == 0:\n            self.speed += 6.0\n        elif action == 1:\n            self.speed -= 6.0\n        elif action == 2:\n            self.altitude += 35.0\n            self.angle += 1.5\n        elif action == 3:\n            self.altitude -= 35.0\n            self.angle -= 1.5\n\n        self.distance -= max(self.speed * 0.3, 1.0)\n        self.altitude -= 8.0\n\n        if self.runway_condition == 0.0:\n            drag_factor = 0.6\n        elif self.runway_condition == 0.5:\n            drag_factor = 0.4\n        else:\n            drag_factor = 0.25\n\n        self.speed -= drag_factor\n        self.angle = np.clip(self.angle, -30, 30)\n        self.altitude = max(self.altitude, 0.0)\n        self.speed = np.clip(self.speed, 0.0, 300.0)\n\n        reward = 0.0\n        reward += (self.prev_distance - self.distance) * 0.02\n        self.prev_distance = self.distance\n        reward -= 0.03\n        reward -= 0.005 * abs(self.altitude - 100)\n        reward -= 0.005 * abs(self.speed - 150)\n        reward -= 0.01 * abs(self.angle)\n\n        if self.distance < 400:\n            reward += 0.8\n        if 0 < self.altitude < 100 and 100 < self.speed < 200:\n            reward += 1.5\n        reward += (self.start_dist - self.distance) / max(1.0, self.start_dist)\n\n        done = False\n        success = False\n        outcome = \"in-flight\"\n\n        if self.distance <= 0:\n            if 0 <= self.altitude <= 50 and 100 <= self.speed <= 200 and abs(self.angle) < 10:\n                reward += 200.0\n                success = True\n                outcome = \"successful landing\"\n            else:\n                reward -= 40.0\n                outcome = \"failed landing\"\n            done = True\n\n        if self.altitude <= 0 and self.distance > 0:\n            reward -= 40.0\n            done = True\n            outcome = \"crash before runway\"\n\n        if self.speed <= 20 and self.altitude > 100:\n            reward -= 40.0\n            done = True\n            outcome = \"stall midair\"\n\n        if self.steps >= 600:\n            done = True\n            outcome = \"timeout\"\n\n        info = {\n            \"success\": success,\n            \"outcome\": outcome,\n            \"runway_condition\": self.runway_condition\n        }\n\n        return self._get_obs(), float(reward), bool(done), False, info\n\n    def _get_obs(self):\n        return np.array([\n            self.altitude / 5000.0,\n            self.speed / 300.0,\n            self.distance / 10000.0,\n            (self.angle + 30.0) / 60.0,\n            self.runway_condition\n        ], dtype=np.float32)\n\n\nclass ActorCritic(nn.Module):\n    def __init__(self, obs_dim, action_dim):\n        super().__init__()\n        self.shared = nn.Sequential(\n            nn.Linear(obs_dim, 256), nn.ReLU(),\n            nn.Linear(256, 256), nn.ReLU()\n        )\n        self.policy = nn.Linear(256, action_dim)\n        self.value = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = self.shared(x)\n        return self.policy(x), self.value(x)\n\n\nclass PPOAgent:\n    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, lam=0.95, clip=0.2, device=\"cpu\"):\n        self.gamma = gamma\n        self.lam = lam\n        self.clip = clip\n        self.device = torch.device(device)\n\n        self.net = ActorCritic(obs_dim, action_dim).to(self.device)\n        self.opt = optim.Adam(self.net.parameters(), lr=lr)\n\n    def act(self, obs):\n        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n        logits, value = self.net(obs_t)\n        probs = torch.softmax(logits, dim=-1)\n        dist = torch.distributions.Categorical(probs)\n        action = dist.sample()\n        return int(action.item()), dist.log_prob(action).item(), float(value.item())\n\n    def compute_advantages(self, rewards, values, dones, last_value):\n        adv = 0\n        advantages = []\n        values = values + [last_value]\n        for t in reversed(range(len(rewards))):\n            td = rewards[t] + self.gamma * values[t + 1] * (1 - (1.0 if dones[t] else 0.0)) - values[t]\n            adv = td + self.gamma * self.lam * (1 - (1.0 if dones[t] else 0.0)) * adv\n            advantages.append(adv)\n        return list(reversed(advantages))\n\n    def update(self, batch, epochs=10, batch_size=64):\n        states = torch.tensor(np.array(batch[\"states\"]), dtype=torch.float32, device=self.device)\n        actions = torch.tensor(batch[\"actions\"], dtype=torch.int64, device=self.device)\n        old_log_probs = torch.tensor(batch[\"log_probs\"], dtype=torch.float32, device=self.device)\n        returns = torch.tensor(batch[\"returns\"], dtype=torch.float32, device=self.device)\n        advantages = torch.tensor(batch[\"advantages\"], dtype=torch.float32, device=self.device)\n\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        dataset_size = len(states)\n        for _ in range(epochs):\n            idxs = np.random.permutation(dataset_size)\n            for i in range(0, dataset_size, batch_size):\n                inds = idxs[i:i + batch_size]\n                logits, value = self.net(states[inds])\n                probs = torch.softmax(logits, dim=-1)\n                dist = torch.distributions.Categorical(probs)\n\n                log_p = dist.log_prob(actions[inds])\n                ratio = torch.exp(log_p - old_log_probs[inds])\n\n                surr1 = ratio * advantages[inds]\n                surr2 = torch.clamp(ratio, 1 - self.clip, 1 + self.clip) * advantages[inds]\n                policy_loss = -torch.min(surr1, surr2).mean()\n\n                value_loss = nn.MSELoss()(value.squeeze(), returns[inds])\n                entropy = dist.entropy().mean()\n                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n\n                self.opt.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)\n                self.opt.step()\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument('--episodes', type=int, default=50000)\n    p.add_argument('--lr', type=float, default=3e-4)\n    p.add_argument('--rollout_steps', type=int, default=2048)\n    p.add_argument('--gamma', type=float, default=0.99)\n    p.add_argument('--lam', type=float, default=0.95)\n    p.add_argument('--clip', type=float, default=0.2)\n    p.add_argument('--save_dir', type=str, default='./checkpoints')\n    p.add_argument('--cpu', action='store_true')\n    p.add_argument('--seed', type=int, default=42)\n    return p.parse_args()\n\n\ndef train(args):\n    device = 'cuda' if torch.cuda.is_available() and not args.cpu else 'cpu'\n    torch.manual_seed(args.seed); np.random.seed(args.seed); random.seed(args.seed)\n\n    env = FlightEnv()\n    obs_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n\n    agent = PPOAgent(obs_dim, action_dim, lr=args.lr, gamma=args.gamma,\n                     lam=args.lam, clip=args.clip, device=device)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # initialize with expected keys but allow unknown outcomes\n    outcomes = {\"successful landing\": 0, \"failed landing\": 0,\n                \"crash before runway\": 0, \"stall midair\": 0, \"timeout\": 0}\n    runway_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n    runway_success = {0.0: 0, 0.5: 0, 1.0: 0}\n\n    best_return = -1e9\n    episode = 0\n\n    while episode < args.episodes:\n        rollout = {\"states\": [], \"actions\": [], \"rewards\": [],\n                   \"values\": [], \"log_probs\": [], \"dones\": []}\n\n        obs, _ = env.reset()\n        last_info = {\"outcome\": \"in-flight\", \"runway_condition\": 0.0, \"success\": False}\n\n        for _ in range(args.rollout_steps):\n            action, logp, value = agent.act(obs)\n            next_obs, reward, done, _, info = env.step(action)\n\n            rollout[\"states\"].append(obs)\n            rollout[\"actions\"].append(action)\n            rollout[\"rewards\"].append(reward)\n            rollout[\"values\"].append(value)\n            rollout[\"log_probs\"].append(logp)\n            rollout[\"dones\"].append(done)\n\n            obs = next_obs\n            last_info = info  # keep last info so we have something after rollout\n\n            # If the env returns done we reset to continue filling the rollout,\n            # but we also may want to record that episode outcome later.\n            if done:\n                obs, _ = env.reset()\n\n        # compute last value for bootstrap\n        with torch.no_grad():\n            last_value = agent.net(\n                torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n            )[1].item()\n\n        advantages = agent.compute_advantages(\n            rollout[\"rewards\"], rollout[\"values\"], rollout[\"dones\"], last_value\n        )\n        returns = [adv + val for adv, val in zip(advantages, rollout[\"values\"])]\n\n        batch = {\n            \"states\": rollout[\"states\"],\n            \"actions\": rollout[\"actions\"],\n            \"log_probs\": rollout[\"log_probs\"],\n            \"advantages\": advantages,\n            \"returns\": returns\n        }\n\n        agent.update(batch)\n\n        # Robust bookkeeping: use last_info but guard unknown keys\n        outcome = last_info.get(\"outcome\", \"in-flight\")\n        # ensure the key exists (no KeyError)\n        outcomes.setdefault(outcome, 0)\n        outcomes[outcome] += 1\n\n        runway_val = float(last_info.get(\"runway_condition\", 0.0))\n        runway_counts.setdefault(runway_val, 0)\n        runway_counts[runway_val] += 1\n\n        if last_info.get(\"success\", False):\n            runway_success.setdefault(runway_val, 0)\n            runway_success[runway_val] += 1\n\n        episode += 1\n        ep_return = sum(rollout[\"rewards\"])\n        if ep_return > best_return:\n            best_return = ep_return\n            torch.save(agent.net.state_dict(), os.path.join(args.save_dir, \"best_model.pt\"))\n\n        if episode % 200 == 0:\n            runway_probs = {\n                k: (runway_success.get(k, 0) / runway_counts.get(k, 1) if runway_counts.get(k, 0) > 0 else 0)\n                for k in sorted(runway_counts.keys())\n            }\n            print(\"\\n--- Outcome Summary up to Episode\", episode, \"---\")\n            for k, v in outcomes.items():\n                print(f\"{k:<25}: {v}\")\n            print(\"Runway success probabilities:\", runway_probs)\n            print(\"------------------------------------------\\n\")\n\n    print(\"Training complete.\")\n    print(\"Best Episode Return:\", best_return)\n    print(\"Final outcomes summary:\")\n    for k, v in outcomes.items():\n        print(f\"{k:<25}: {v}\")\n\n\nif __name__ == \"__main__\":\n    import sys\n    sys.argv = [sys.argv[0]]\n    args = parse_args()\n    print(\"Starting PPO training with args:\", args)\n    train(args)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:16:09.934900Z","iopub.execute_input":"2025-11-14T09:16:09.935098Z","execution_failed":"2025-11-14T09:17:38.493Z"}},"outputs":[],"execution_count":null}]}